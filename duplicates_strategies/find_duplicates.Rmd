---
title: "Duplicates"
author: "Laura Menicacci"
date: "2023-01-13"
output: html_document
---

```{r}
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(caret)
```

## Load data 

```{r}
ceres <- read_csv("C:\\Users\\laura\\OneDrive\\Documenti\\LAURA\\MCC\\futurelab_ceres\\ceres_versions\\ceres_thirdversion.csv")

lab_data <- read_csv("C:\\Users\\laura\\OneDrive\\Documenti\\LAURA\\MCC\\Policy databases\\duplicates_testsample.csv")

```

# Detection of IEA and CPD duplicates 
```{r}
ceres %>% group_by(indicator_database_source) %>% count() # 1544 UNFCCC in total - 1653 CPD +IEA
  
```
# Cosine similarity
```{r}
lab_data <- lab_data %>% 
  select(!`@dropdown`) %>% 
  tibble::rowid_to_column("index")

corp <- corpus(lab_data$policy_name) 
docnames(corp) <- paste(lab_data$index)

dfmat <- corp %>% tokens(remove_punc=TRUE) %>%
  dfm()

sim_mat <- textstat_simil(dfmat, method="cosine")
sim_df <- as.data.frame(sim_mat, upper=TRUE)

```

# filter by threshold and label dataset

We label the dataset by inserting the index of similar policies as values in the column cos_dup. In this way, it is easier to inspect manually which policies are similar to which. 

(Is there a more efficient way to do this???? )
```{r}
sim_filt <- sim_df %>% filter(cosine >= 0.7) # threshold for duplicate

lab_copy <- lab_data # create a copy 
lab_copy$cos_dup <- NA

# labelling 

for (i in 1:(nrow(sim_filt))) {  
  index1 <- as.numeric(sim_filt[i, 2])
  if (is.na(lab_copy$cos_dup[i])) {lab_copy$cos_dup[index1] <- paste(lab_copy$cos_dup[index1], as.numeric(sim_filt[i,1]), sep = ",")} 
  else {lab_copy$cos_dup[index1] <- paste(lab_copy$cos_dup[index1], as.numeric(sim_filt[i,1]), sep = ",")}
}

lab_copy$cos_dup <- trimws(lab_copy$cos_dup, whitespace = "NA,")

```

# Check performance - Accuracy score

74% accuracy without filtering for country and year - 60% cosine similarity threshold
```{r}
lab_copy_scores <- lab_copy %>% 
  mutate(cos_dup_n = ifelse(is.na(cos_dup) == F, 1, 0)) %>% 
  mutate(tp = duplicate == "yes" & is.na(cos_dup) == F) %>% 
  mutate(tn = duplicate == "no" & is.na(cos_dup) == T) %>% 
  mutate(fn = duplicate == "yes" & is.na(cos_dup) == T) %>% 
  mutate(fp = duplicate == "no" & is.na(cos_dup) == F) %>% 
  mutate(duplicate = ifelse(duplicate == "yes", 1, 0))
  
lab_copy_scores$tp <- ifelse(lab_copy_scores$tp == T, 1, 0)
lab_copy_scores$tn <- ifelse(lab_copy_scores$tn == T, 1, 0)
lab_copy_scores$fn <- ifelse(lab_copy_scores$fn == T, 1, 0)
lab_copy_scores$fp <- ifelse(lab_copy_scores$fp == T, 1, 0)


confmat <- table(lab_copy_scores$duplicate, lab_copy_scores$cos_dup_n) #confusion matrix

print(confmat)

# Accuracy = (TP+TN)/(TP+TN+FP+FN)

accuracy <- (53 +74)/ (53+74+19+13) # to do better!!! con tp tn fn fp

print(accuracy) 

```

# inspection of results

```{r}
# plot fn and fp ?? 
```


# Label dupes using cosine similarity with country and years filter/ weight?? 

```{r}
# check row indexed in dup col -> if == then ?? 

lab_copy$country_check <- NA

for (i in 1:nrow(lab_copy)) {
  index2 <- lab_copy[i, 14]
  if (is.na(lab_copy[i, 14] == T)) {lab_copy$country_check <- NA}
  if (is.na(lab_copy[i, 14] == F)) {if (lab_copy[[i, 2]] == lab_copy[[as.numeric(index2), 2]]) {lab_copy$country_check[i] <- 1}}
  else if (is.na(lab_copy[i, 14] == F)) {if (lab_copy[[i, 2]] == lab_copy[[as.numeric(index2), 2]]) {lab_copy$country_check[i] <- 0}}
}


for (i in 1:nrow(lab_copy)) {
  index2 <- lab_copy[[i, 14]]
  if (is.na(lab_copy[i, 14])) next
  else if (lab_copy[[i, 2]] == lab_copy[[as.numeric(index2), 2]]) {lab_copy$country_check[i] <- 1}
  else if (lab_copy[[i, 2]] != lab_copy[[as.numeric(index2), 2]]) {lab_copy$country_check[i] <- 0}
  }


```



# CERES try 

```{r}
lab_d <- ceres %>% 
  select(policy_name, country, indicator_database_source) %>% 
  #filter(indicator_database_source == c(""))
  group_by(country) %>% 
  tibble::rowid_to_column("index")

corp <- corpus(lab_d$policy_name) 
docnames(corp) <- paste(lab_d$policy_name, lab_d$country, sep = " ")

dfmat <- corp %>% tokens(remove_punc=TRUE) %>%
  dfm()

sim_mat <- textstat_simil(dfmat, method="cosine")
sim_df <- as.data.frame(sim_mat, upper=TRUE)

sim_filt <- sim_df %>% filter(cosine >= 0.6) #threshold for duplicate

lab_copy <- ceres # CHANGE AFTER
lab_copy$cos_dup <- NA

for (i in 1:(nrow(sim_filt))) {
  index1 <- as.numeric(sim_filt[i, 2])
  if (is.na(lab_copy$cos_dup[i]) == T) {lab_copy$cos_dup[index1] <- paste(lab_copy$cos_dup[index1], as.numeric(sim_filt[i,1]), sep = ",")} 
  else {lab_copy$cos_dup[index1] <- paste(lab_copy$cos_dup[index1], as.numeric(sim_filt[i,1]), sep = ",")}
}

lab_copy$cos_dup <- trimws(lab_copy$cos_dup, whitespace = "NA,")

lab_copy %>% 
  mutate(cos_dup_n = ifelse(is.na(cos_dup) == F, 1, 0)) %>% 
  count(cos_dup_n) # 2133 detected duplicates out of 3492


```

################################################################################

```{r}
dup <- read_csv("C:\\Users\\laura\\OneDrive\\Documenti\\LAURA\\MCC\\Policy databases\\duplicates_1.csv")

filt_dup <- dup %>% filter(`Country Code` %in% c("AUS", "AUT","BEL", "CAN", "DNK", "FIN", "FRA", "DEU", "GRC",
                      "IRL", "ITA", "JPN", "LUX", "NLD", "NZL", "PRT", "ESP", "SWE",
                      "CHE", "GBR", "USA"))

filt_dup %>% group_by(indicator_database_source) %>% count()

uni <- unique(filt_dup$`Policy Name`)
uni 
```


