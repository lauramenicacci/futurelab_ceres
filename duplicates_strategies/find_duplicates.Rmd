---
title: "Duplicates"
author: "Laura Menicacci"
date: "2023-01-13"
output: html_document
---

```{r}
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(caret)
```

## Load data 

```{r}
ceres <- read_csv("C:\\Users\\laura\\OneDrive\\Documenti\\LAURA\\MCC\\futurelab_ceres\\ceres_versions\\ceres_thirdversion.csv")

lab_data <- read_csv("C:\\Users\\laura\\OneDrive\\Documenti\\LAURA\\MCC\\Policy databases\\duplicates_testsample.csv")

```

# Detection of IEA and CPD duplicates 
```{r}
ceres %>% group_by(indicator_database_source) %>% count() # 1544 UNFCCC in total - 1653 CPD +IEA
  
```
# Cosine similarity
```{r}
lab_data <- lab_data %>% 
  select(!`@dropdown`) %>% 
  tibble::rowid_to_column("index")

corp <- corpus(lab_data$policy_name) 
docnames(corp) <- paste(lab_data$index)

dfmat <- corp %>% tokens(remove_punc=TRUE) %>%
  dfm()

sim_mat <- textstat_simil(dfmat, method="cosine")
sim_df <- as.data.frame(sim_mat, upper=TRUE)

```

# Filter by cosine threshold, filter duplicates by country, label

We first remove the duplicates that have high score but different countries, since they are not duplicates. Then we label the dataset by inserting the index of similar policies as values in the column cos_dup. In this way, it is easier to inspect manually which policies are similar to which. 

```{r}
sim_filt <- sim_df %>% filter(cosine >= 0.7) # threshold for duplicate

lab_copy <- lab_data # create a copy 
lab_copy$cos_dup <- NA

for (i in 1:nrow(sim_filt)) {        # remove duplicates with different countries
  index1 = as.numeric(sim_filt[i,1])
  index2 = as.numeric(sim_filt[i,2])
  if(lab_copy[index1,2]!=lab_copy[index2,2]){
    sim_filt[i, 3] = NA
    sim_filt_2 <- na.omit(sim_filt)
    }
}
for (i in 1:nrow(sim_filt_2)){       # labeling 
  index1 = as.numeric(sim_filt_2[i,1])
  lab_copy[index1,14] = paste(lab_copy[index1,14], sim_filt_2[i,2], sep = ",")}

lab_copy$cos_dup <- trimws(lab_copy$cos_dup, whitespace = "NA,")


lab_copy$year_diff <- NA

for (i in 1:nrow(sim_filt_2)) {      # add indicator for duplicates with differing years
  index1 = as.numeric(sim_filt_2[i,1])
  index2 = as.numeric(sim_filt_2[i,2])
  if (is.na(lab_copy[i, 12])) {lab_copy$year_diff <- NA} 
  else {if (lab_copy[[index1,12]]!= lab_copy[[index2,12]]) {lab_copy[index1, 15] <- "year diff"}}
} # DOESN'T WORK BECAUSE THERE ARE NAs IN YEAR_ALL --> try overcoming with TRYCATCH ??? 
  
```

# Check performance of cosine similarity - accuracy score

74% accuracy without filtering for country and year - 60% cosine similarity threshold
```{r}
lab_copy_scores <- lab_copy %>% 
  mutate(cos_dup_n = ifelse(is.na(cos_dup) == F, 1, 0)) %>% 
  mutate(tp = duplicate == "yes" & is.na(cos_dup) == F) %>% 
  mutate(tn = duplicate == "no" & is.na(cos_dup) == T) %>% 
  mutate(fn = duplicate == "yes" & is.na(cos_dup) == T) %>% 
  mutate(fp = duplicate == "no" & is.na(cos_dup) == F) %>% 
  mutate(duplicate = ifelse(duplicate == "yes", 1, 0))
  
lab_copy_scores$tp <- ifelse(lab_copy_scores$tp == T, 1, 0)
lab_copy_scores$tn <- ifelse(lab_copy_scores$tn == T, 1, 0)
lab_copy_scores$fn <- ifelse(lab_copy_scores$fn == T, 1, 0)
lab_copy_scores$fp <- ifelse(lab_copy_scores$fp == T, 1, 0)


confmat <- table(lab_copy_scores$duplicate, lab_copy_scores$cos_dup_n) #confusion matrix

print(confmat)

# Accuracy = (TP+TN)/(TP+TN+FP+FN)

accuracy <- (57+72)/ (57 +72+15+15) 

print(accuracy) 

```

# Inspection of results

```{r}

lab_copy_scores %>% group_by(country) %>% 
  ggplot(aes(x = country, y = fn)) + geom_col() + ggtitle("Frequency of false negatives - duplicates not detected") + theme_bw()

lab_copy_scores %>% group_by(country) %>% 
  ggplot(aes(x = country, y = fp)) + geom_col() + ggtitle("Frequency of false positives - fake duplicates detected") + theme_bw()

lab_copy_scores %>% group_by(country) %>% 
  mutate(errors = fp+fn) %>% 
  ggplot(aes(x = country, y = errors)) + geom_col() + ggtitle("Error rate per country") + theme_bw()

```
# Remove/merge duplicates
```{r}

 


```


# Cosine similarity between policies of IEA and CPD 
```{r}
#sim_new = sim_df
#
#for (i in 1:nrow(sim_new)) {
#  if (sim_new[i, 3] < 0.70){
#    sim_new[i, 3] = NA
#    sim_removed <- na.omit(sim_new)
#  }
#}
#
#
#for (i in 1:(nrow(sim_filt_2))) {  
#  index1 <- as.numeric(sim_filt_2[i, 2])
#  if (is.na(lab_copy$cos_dup[i])) {lab_copy$cos_dup[index1] <- paste(lab_copy$cos_dup[index1], #as.numeric(sim_filt_2[i,1]), sep = ",")} 
#  else {lab_copy$cos_dup[index1] <- paste(lab_copy$cos_dup[index1], as.numeric(sim_filt_2[i,1]), sep = ",")}
#}

```


# CERES try 

```{r}
lab_data <- ceres %>% 
  tibble::rowid_to_column("index")

corp <- corpus(lab_data$policy_name) 
docnames(corp) <- paste(lab_data$index)

dfmat <- corp %>% tokens(remove_punc=TRUE) %>%
  dfm()

sim_mat <- textstat_simil(dfmat, method="cosine")
sim_df <- as.data.frame(sim_mat, upper=TRUE)

sim_filt <- sim_df %>% filter(cosine >= 0.7) # threshold for duplicate

lab_copy <- lab_data # create a copy 
lab_copy$cos_dup <- NA

for (i in 1:nrow(sim_filt)) {        # remove duplicates with different countries
  index1 = as.numeric(sim_filt[i,1])
  index2 = as.numeric(sim_filt[i,2])
  if(lab_copy[index1,2]!=lab_copy[index2,2]){
    sim_filt[i, 3] = NA
    sim_filt_2 <- na.omit(sim_filt)
    }
}
for (i in 1:nrow(sim_filt_2)){       # labeling 
  index1 = as.numeric(sim_filt_2[i,1])
  lab_copy[index1,12] = paste(lab_copy[index1,12], sim_filt_2[i,2], sep = ",")}

lab_copy$cos_dup <- trimws(lab_copy$cos_dup, whitespace = "NA,")

lab_copy %>% 
  mutate(cos_dup_n = ifelse(is.na(cos_dup) == F, 1, 0)) %>% 
  count(cos_dup_n) # 1274 detected duplicates out of 3492


```

################################################################################

```{r}
dup <- read_csv("C:\\Users\\laura\\OneDrive\\Documenti\\LAURA\\MCC\\Policy databases\\duplicates_1.csv")

filt_dup <- dup %>% filter(`Country Code` %in% c("AUS", "AUT","BEL", "CAN", "DNK", "FIN", "FRA", "DEU", "GRC",
                      "IRL", "ITA", "JPN", "LUX", "NLD", "NZL", "PRT", "ESP", "SWE",
                      "CHE", "GBR", "USA"))

filt_dup %>% group_by(indicator_database_source) %>% count()

uni <- unique(filt_dup$`Policy Name`)
uni 
```


